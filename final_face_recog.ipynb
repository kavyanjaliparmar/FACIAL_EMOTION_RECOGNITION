{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd221419-80a8-4000-b409-acb01722be4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry done\n",
      "disgust done\n",
      "fear done\n",
      "happy done\n",
      "neutral done\n",
      "sad done\n",
      "surprise done\n",
      "                                                   image     label\n",
      "0      C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...     angry\n",
      "1      C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...     angry\n",
      "2      C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...     angry\n",
      "3      C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...     angry\n",
      "4      C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...     angry\n",
      "...                                                  ...       ...\n",
      "28704  C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...  surprise\n",
      "28705  C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...  surprise\n",
      "28706  C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...  surprise\n",
      "28707  C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...  surprise\n",
      "28708  C:\\Users\\user\\Downloads\\fer2013_dataset\\train\\...  surprise\n",
      "\n",
      "[28709 rows x 2 columns]\n",
      "angry done\n",
      "disgust done\n",
      "fear done\n",
      "happy done\n",
      "neutral done\n",
      "sad done\n",
      "surprise done\n",
      "                                                  image     label\n",
      "0     C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\a...     angry\n",
      "1     C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\a...     angry\n",
      "2     C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\a...     angry\n",
      "3     C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\a...     angry\n",
      "4     C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\a...     angry\n",
      "...                                                 ...       ...\n",
      "7173  C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\s...  surprise\n",
      "7174  C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\s...  surprise\n",
      "7175  C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\s...  surprise\n",
      "7176  C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\s...  surprise\n",
      "7177  C:\\Users\\user\\Downloads\\fer2013_dataset\\test\\s...  surprise\n",
      "\n",
      "[7178 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 28709/28709 [02:06<00:00, 226.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 7178/7178 [01:01<00:00, 117.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
    "from keras_preprocessing.image import load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "train_dir = r\"C:\\Users\\user\\Downloads\\fer2013_dataset\\train\"\n",
    "test_dir = r\"C:\\Users\\user\\Downloads\\fer2013_dataset\\test\"\n",
    "                         \n",
    "def dataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir, label)):\n",
    "            image_paths.append(os.path.join(dir, label, imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"done\")\n",
    "    return image_paths, labels\n",
    "\n",
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = dataframe(train_dir)\n",
    "\n",
    "print(train)\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = dataframe(test_dir)\n",
    "\n",
    "print(test)\n",
    "\n",
    "def features(images):\n",
    "    feat = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, color_mode=\"grayscale\")\n",
    "        img = np.array(img)\n",
    "        feat.append(img)\n",
    "    feat = np.array(feat)\n",
    "    feat = feat.reshape(len(feat), 48, 48, 1)\n",
    "    return feat\n",
    "\n",
    "\n",
    "train_features = features(train['image'])\n",
    "test_features = features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13500f2f-18ec-41af-872a-44b62e33c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_features/255.0\n",
    "x_test= test_features/255.0\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "le.fit(train['label'])\n",
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])\n",
    "y_train = to_categorical(y_train, num_classes = 7)\n",
    "y_test = to_categorical (y_test, num_classes = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa220e6d-6cb4-49b9-b020-c483554ffac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc06d6ea-4b75-46e8-9b84-384eb57a3e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 2s/step - accuracy: 0.2366 - loss: 1.8362 - val_accuracy: 0.2510 - val_loss: 1.7953\n",
      "Epoch 2/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 2s/step - accuracy: 0.2486 - loss: 1.8012 - val_accuracy: 0.3040 - val_loss: 1.6992\n",
      "Epoch 3/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 2s/step - accuracy: 0.3014 - loss: 1.7143 - val_accuracy: 0.3689 - val_loss: 1.5734\n",
      "Epoch 4/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.3856 - loss: 1.5677 - val_accuracy: 0.4549 - val_loss: 1.3941\n",
      "Epoch 5/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 3s/step - accuracy: 0.4323 - loss: 1.4641 - val_accuracy: 0.4834 - val_loss: 1.3386\n",
      "Epoch 6/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2509s\u001b[0m 11s/step - accuracy: 0.4627 - loss: 1.4004 - val_accuracy: 0.5052 - val_loss: 1.2860\n",
      "Epoch 7/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 3s/step - accuracy: 0.4769 - loss: 1.3634 - val_accuracy: 0.5233 - val_loss: 1.2450\n",
      "Epoch 8/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 3s/step - accuracy: 0.4964 - loss: 1.3256 - val_accuracy: 0.5162 - val_loss: 1.2479\n",
      "Epoch 9/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 4s/step - accuracy: 0.4990 - loss: 1.3013 - val_accuracy: 0.5405 - val_loss: 1.1992\n",
      "Epoch 10/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 3s/step - accuracy: 0.5098 - loss: 1.2787 - val_accuracy: 0.5418 - val_loss: 1.2025\n",
      "Epoch 11/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 2s/step - accuracy: 0.5214 - loss: 1.2680 - val_accuracy: 0.5496 - val_loss: 1.1748\n",
      "Epoch 12/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 2s/step - accuracy: 0.5230 - loss: 1.2456 - val_accuracy: 0.5483 - val_loss: 1.1613\n",
      "Epoch 13/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 2s/step - accuracy: 0.5303 - loss: 1.2345 - val_accuracy: 0.5527 - val_loss: 1.1543\n",
      "Epoch 14/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10447s\u001b[0m 47s/step - accuracy: 0.5355 - loss: 1.2150 - val_accuracy: 0.5635 - val_loss: 1.1397\n",
      "Epoch 15/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 2s/step - accuracy: 0.5430 - loss: 1.2011 - val_accuracy: 0.5658 - val_loss: 1.1292\n",
      "Epoch 16/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2626s\u001b[0m 12s/step - accuracy: 0.5475 - loss: 1.1855 - val_accuracy: 0.5726 - val_loss: 1.1215\n",
      "Epoch 17/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 3s/step - accuracy: 0.5600 - loss: 1.1590 - val_accuracy: 0.5726 - val_loss: 1.1126\n",
      "Epoch 18/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 3s/step - accuracy: 0.5632 - loss: 1.1535 - val_accuracy: 0.5766 - val_loss: 1.1022\n",
      "Epoch 19/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2968s\u001b[0m 13s/step - accuracy: 0.5621 - loss: 1.1514 - val_accuracy: 0.5811 - val_loss: 1.0928\n",
      "Epoch 20/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m756s\u001b[0m 3s/step - accuracy: 0.5670 - loss: 1.1501 - val_accuracy: 0.5740 - val_loss: 1.0963\n",
      "Epoch 21/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 3s/step - accuracy: 0.5708 - loss: 1.1310 - val_accuracy: 0.5869 - val_loss: 1.0959\n",
      "Epoch 22/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 2s/step - accuracy: 0.5666 - loss: 1.1393 - val_accuracy: 0.5885 - val_loss: 1.0782\n",
      "Epoch 23/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 3s/step - accuracy: 0.5800 - loss: 1.1136 - val_accuracy: 0.5880 - val_loss: 1.0784\n",
      "Epoch 24/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 2s/step - accuracy: 0.5780 - loss: 1.1059 - val_accuracy: 0.5885 - val_loss: 1.0792\n",
      "Epoch 25/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 2s/step - accuracy: 0.5864 - loss: 1.1005 - val_accuracy: 0.5936 - val_loss: 1.0674\n",
      "Epoch 26/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1052s\u001b[0m 5s/step - accuracy: 0.5906 - loss: 1.0848 - val_accuracy: 0.5900 - val_loss: 1.0759\n",
      "Epoch 27/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 2s/step - accuracy: 0.5956 - loss: 1.0684 - val_accuracy: 0.5940 - val_loss: 1.0696\n",
      "Epoch 28/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 2s/step - accuracy: 0.5998 - loss: 1.0629 - val_accuracy: 0.5971 - val_loss: 1.0536\n",
      "Epoch 29/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 3s/step - accuracy: 0.6013 - loss: 1.0464 - val_accuracy: 0.6018 - val_loss: 1.0519\n",
      "Epoch 30/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 3s/step - accuracy: 0.5998 - loss: 1.0554 - val_accuracy: 0.6053 - val_loss: 1.0455\n",
      "Epoch 31/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1655s\u001b[0m 7s/step - accuracy: 0.6143 - loss: 1.0373 - val_accuracy: 0.6017 - val_loss: 1.0514\n",
      "Epoch 32/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2691s\u001b[0m 12s/step - accuracy: 0.6100 - loss: 1.0370 - val_accuracy: 0.6035 - val_loss: 1.0553\n",
      "Epoch 33/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33157s\u001b[0m 148s/step - accuracy: 0.6106 - loss: 1.0365 - val_accuracy: 0.6066 - val_loss: 1.0441\n",
      "Epoch 34/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 2s/step - accuracy: 0.6088 - loss: 1.0396 - val_accuracy: 0.6013 - val_loss: 1.0483\n",
      "Epoch 35/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 2s/step - accuracy: 0.6172 - loss: 1.0101 - val_accuracy: 0.6116 - val_loss: 1.0321\n",
      "Epoch 36/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 2s/step - accuracy: 0.6184 - loss: 1.0146 - val_accuracy: 0.6028 - val_loss: 1.0562\n",
      "Epoch 37/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3867s\u001b[0m 17s/step - accuracy: 0.6192 - loss: 1.0122 - val_accuracy: 0.6017 - val_loss: 1.0498\n",
      "Epoch 38/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 2s/step - accuracy: 0.6256 - loss: 0.9952 - val_accuracy: 0.6134 - val_loss: 1.0409\n",
      "Epoch 39/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8915s\u001b[0m 40s/step - accuracy: 0.6258 - loss: 1.0020 - val_accuracy: 0.6060 - val_loss: 1.0470\n",
      "Epoch 40/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 2s/step - accuracy: 0.6332 - loss: 0.9853 - val_accuracy: 0.6067 - val_loss: 1.0377\n",
      "Epoch 41/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m882s\u001b[0m 4s/step - accuracy: 0.6341 - loss: 0.9802 - val_accuracy: 0.6140 - val_loss: 1.0269\n",
      "Epoch 42/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 2s/step - accuracy: 0.6348 - loss: 0.9766 - val_accuracy: 0.6183 - val_loss: 1.0280\n",
      "Epoch 43/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4309s\u001b[0m 19s/step - accuracy: 0.6336 - loss: 0.9700 - val_accuracy: 0.6137 - val_loss: 1.0325\n",
      "Epoch 44/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1063s\u001b[0m 5s/step - accuracy: 0.6380 - loss: 0.9618 - val_accuracy: 0.6116 - val_loss: 1.0308\n",
      "Epoch 45/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 3s/step - accuracy: 0.6435 - loss: 0.9492 - val_accuracy: 0.6128 - val_loss: 1.0366\n",
      "Epoch 46/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 3s/step - accuracy: 0.6412 - loss: 0.9673 - val_accuracy: 0.6197 - val_loss: 1.0197\n",
      "Epoch 47/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 2s/step - accuracy: 0.6485 - loss: 0.9522 - val_accuracy: 0.6130 - val_loss: 1.0325\n",
      "Epoch 48/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1269s\u001b[0m 6s/step - accuracy: 0.6526 - loss: 0.9321 - val_accuracy: 0.6128 - val_loss: 1.0310\n",
      "Epoch 49/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 2s/step - accuracy: 0.6485 - loss: 0.9477 - val_accuracy: 0.6190 - val_loss: 1.0257\n",
      "Epoch 50/50\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11086s\u001b[0m 49s/step - accuracy: 0.6594 - loss: 0.9153 - val_accuracy: 0.6212 - val_loss: 1.0203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x208605924d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.fit(x=x_train, y= y_train, batch_size = 128, epochs = 50, validation_data = (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c899e9-23cb-4809-bd3b-b611f1444065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'face.ipynb', 'facevideo.ipynb', 'face_detection_model.h5', 'face_detection_model.keras', 'fer2013_dataset', 'haarcascade_frontalface_default.xml', \"kavya's emotion detector.h5\", \"kavya's emotion detector.keras\", 'kavyas_emotion_detector.h5', 'kavya_emotion_detector.keras', 'model.h5', 'model_augmented.h5', 'Untitled.ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb', 'updatedFace.ipynb', 'using_utk.ipynb']\n"
     ]
    }
   ],
   "source": [
    "model.save(\"kavya_emotion_detector.keras\")\n",
    "import os\n",
    "print(os.listdir(r\"C:\\Users\\user\\face recognition\"))  \n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "model = load_model(\"kavya_emotion_detector.keras\")\n",
    "label = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
    "\n",
    "def ef(image):\n",
    "    img=load_img(image,grayscale = True)\n",
    "    feature=np.array(img)\n",
    "    feature=feature.reshape(1,48,48,1)\n",
    "    return feature/255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60b7f453-f082-404c-bb84-f0293a787dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"kavya_emotion_detector.keras\")\n",
    "label = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
    "\n",
    "def rectangle_boundary(image, classifier, scaleFactor=1.3, minNeighbors=8, color=None):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    features = classifier.detectMultiScale(gray_image, scaleFactor, minNeighbors)\n",
    "    coordinates = []\n",
    "    for (x, y, w, h) in features:\n",
    "        coordinates.append((x, y, w, h))\n",
    "    if coordinates:\n",
    "        coordinates = sorted(coordinates, key=lambda x: x[2] * x[3], reverse=True)[:1]\n",
    "        for (x, y, w, h) in coordinates:\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 3)\n",
    "    return coordinates, image\n",
    "\n",
    "def preprocess_face(face_img):\n",
    "    face_img = cv2.resize(face_img, (48, 48))\n",
    "    face_img = face_img / 255.0\n",
    "    face_img = np.expand_dims(face_img, axis=0)\n",
    "    face_img = np.expand_dims(face_img, axis=-1)\n",
    "    return face_img\n",
    "\n",
    "def detect(image, faceCascade):\n",
    "    color = {\n",
    "        'white': (255, 255, 255),\n",
    "        'red': (255, 0, 0),\n",
    "        'black': (0, 0, 0)\n",
    "    }\n",
    "    coordinates, image = rectangle_boundary(image, faceCascade, 1.1, 6, color['white'])\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    for (x, y, w, h) in coordinates:\n",
    "        face_img = gray_image[y:y + h, x:x + w]\n",
    "        preprocessed_face = preprocess_face(face_img)\n",
    "        pred = model.predict(preprocessed_face, verbose=0)\n",
    "        emotion_label = label[np.argmax(pred)]\n",
    "        cv2.putText(image, emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "video_cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = video_cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture\")\n",
    "        break\n",
    "    frame = detect(frame, faceCascade)\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('e'):\n",
    "        break\n",
    "\n",
    "video_cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d61a2-7cde-4644-ac54-d4d10d45e545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
